---
title: "[boostcamp AI Tech] í•™ìŠµê¸°ë¡ day26 (week6)"
date: 2021-09-07 20:00:00 -0400
categories:
use_math: true
---

# Image Classification
## Problem by deeper layer
* Deeper network, better performance
    * Large receptive field
    * More capacity and non-linearity
* Gradient vanishing / exploding

* Computationally complex

* Degradation problem
    * ì²˜ìŒì—ëŠ” í‘œí˜„ë ¥ì´ ê³¼í•˜ê²Œ ì¢‹ì•„ì„œ Overfittingì´ ì¼ì–´ë‚ ê²ƒì´ë¼ê³  íŒë‹¨
    * í˜„ì¬ëŠ” Degradation problemë¼ëŠ” ê²ƒì´ ë°œê²¬

## CNN architecture for image classification
3. GoogLeNet
    * Inception module
        * Architecture
            * 1x1, 3x3, 5x5 conv
            * 3x3 pooling
            * concat outputs along channel axis
        * 1x1 conv
            * ë§ì€ í•„í„°ë¡œ ì¸í•˜ì—¬ ê³„ì‚°ë³µì¡ë„ê°€ ì¦ê°€
            * 1x1 convë¥¼ ì´ìš©í•˜ì—¬ channel demension ì¶•ì†Œ
            * ê³„ì‚° ë³µì¡ë„ ê°ì†Œ
    * stem network: vanila CNN
    * stacked inception modules
    * Auxiliary classifiers
        * Gradient vanishing ë¬¸ì œ í•´ê²°
        * lower layerì— ìƒˆë¡œìš´ gradientë¥¼ ì¶”ê°€
        * testì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ

4. ResNet
    * Deeper layerë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ
    * Degradation problem
        * í•™ìŠµì„ í•¨ì— ë”°ë¼ training errorì™€ test errorê°€ saturationë¨
        * overfitting ë¬¸ì œë¼ë©´ training errorê°€ deeper CNNì´ lower CNNë³´ë‹¤ ë‚®ì•„ì ¸ì•¼í•¨
        * ìµœì í™”ì˜ ë¬¸ì œì´ë‹¤! (like Gradient vanishing, exploding)
    * Residual Block
        * Residual function: $H(x)=F(x)+x$
        * Target function: $F(x)=H(x)-x$
        * $H(x)$ë¥¼ ë°”ë¡œ í•™ìŠµí•˜ê²Œ í•˜ëŠ”ê²ƒì´ ì•„ë‹ˆë¼ $H(x)=F(x)+x$ë¡œ ë‘ì–´ residualë§Œ í•™ìŠµí•¨ìœ¼ë¡œì¨ ê¹Šì–´ì ¸ë„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.
        * shortcut or skip connectionì„ í†µí•´ Residual function êµ¬í˜„
        * Analysis
            * $2^{n}$ì˜ ê²½ìš°ì˜ ìˆ˜ê°€ gradientê°€ ì§€ë‚˜ê°€ëŠ” input ouputì„ ë§Œë“œëŠ” ë°©ë²•ì´ë‹¤.
            * ë‹¤ì–‘í•œ ê²½ë¡œë¥¼ í†µí•´ì„œ ë³µì¡í•œ mappingì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.
    * He initialization: Residual blockì— ì í•©í•œ initilization ë°©ë²•

5. DenseNet
    * Dense block
        * ë°”ë¡œ ì§ì „ blockì˜ ì •ë³´ ë¿ë§Œ ì•„ë‹ˆë¼ ë” ì´ì „ layerì˜ ì •ë³´ë¥¼ denseí•˜ê²Œ ì „ë‹¬í•œë‹¤.
        * ë” ë³µì¡í•œ mappingì— í•™ìŠµ
        * Channel axisë¡œ concat
            * + ì‹ í˜¸ì˜ ê²°í•© (ResNet)
            * Concat: ì •ë³´ë¥¼ ê·¸ëŒ€ë¡œ ë³´ì¡´
    * Vanishing gradient ë¬¸ì œë¥¼ ê°ì†Œ
    * Feature propagationì„ ê°•í™”
    * Featureì˜ ì¬ì‚¬ìš©

6. SENet
    * í˜„ì¬ ì£¼ì–´ì§„ activationê°„ì˜ ê´€ê³„ê°€ ëª…í™•í•´ì§€ë„ë¡ chanelê°„ì˜ ê´€ê³„ë¥¼ ëª¨ë¸ë§, ì¤‘ìš”ë„ë¥¼ íŒŒì•…, ì¤‘ìš”í•œ íŠ¹ì§•ì„ attention
    * SE
        * Squeeze: global avg poolingì„í†µí•´ ê³µê°„ì •ë³´ ì œê±° í›„, ë¶„í¬ë¡œ ë³€í™˜
        * Excitation: FC layerë¥¼ ì´ìš©í•˜ì—¬ ì²´ë„ì˜ attention scoreë¥¼ ìƒì„±
        * Attention scoreë¥¼ ì´ìš©í•˜ì—¬ ì¤‘ìš”ë„ì— ë”°ë¼ weightë¥¼ ê³±í•œë‹¤.

7. EfficientNet
    * ì„±ëŠ¥ì„ ë†’ì´ëŠ” ìš”ì†Œ
        * Width scaling: channel ì¶• í™•ì¥
        * Depth scling: deeper
        * Resolution scaling: input image resoultion ì¦ê°€
    * compund scaling
        * width, depth, resulution sclingì„ ì¢…í•©

8. Deformable conv
    * irregular conv
        * ì‚¬ëŒì´ë‚˜ ìë™ì°¨ë“± ë°©í–¥ì´ë‚˜ ì›€ì§ì„ì— ë”°ë¼ì„œ ìƒëŒ€ì ì¸ í˜•íƒœê°€ ë³€í•˜ëŠ” objectì— ëŒ€í•˜ì—¬ deformableí•œ ê²ƒì„ ê³ ë ¤í•˜ê¸° ìœ„í•´ ì œì•ˆ
    * ì „í˜•ì ì¸ convì— ëŒ€í•˜ì–´ offset filedì— ë”°ë¼ì„œ wë¥¼ ë²Œë ¤ì£¼ê²Œ ë˜ê³  irregularly sampling

### Summary
1. AlexNet: Simple architectur, lower layers, heavy memory size, low accuracy
2. VGGNet: simple 2x2 conv architecture, deeper than AlexNet, highest memory, heaviest computation
3. GoogLeNet: inception module and auxilary classifier
4. ResNet: Deeper layer with residual block, moderate efficiency
* CNN backnones
    * GoogLeNetì´ ë‹¤ë¥¸ model ë³´ë‹¤ íš¨ìœ¨ì ì´ì§€ë§Œ êµ¬ì¡°ì˜ ë³µì¡í•¨ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ì–´ë ¤ì›€
    * ì‹¬í”Œí•œ VGGNetì´ë‚˜ ResNet ë“±ì„ backboneìœ¼ë¡œ ì‚¬ìš©

# ê³¼ì œ

# í”¼ì–´ì„¸ì…˜
âœ… ì˜¤ëŠ˜ì˜ í”¼ì–´ì„¸ì…˜ (ëª¨ë”ë ˆì´í„°: ê¹€í˜„ìˆ˜)

1. ê°•ì˜ ìš”ì•½
    - ë°œí‘œì: ê¹€í˜„ìˆ˜
    - ë‚´ìš©: Image classification - 1
    - cnn from scratch : [https://setosa.io/ev/image-kernels/](https://setosa.io/ev/image-kernels/)
    - haar cascading

ğŸ“¢ ë‚´ì¼ ê°ì í•´ì˜¬ ê²ƒ

1. ëª¨ë”ë ˆì´í„°: ë°±ì¢…ì› - ê°•ì˜ 2 ê°•, Data Viz
2. í•„ìˆ˜ ê³¼ì œ ë¦¬ë·°, ì§ˆë¬¸

ğŸ“¢Â ë‚´ì¼ ìš°ë¦¬ íŒ€ì´ í•´ì•¼ í•  ì¼

1. í†¡ë°© ì´ìš©í•œ ì§ˆë¬¸ í™•ì¸

ğŸ“¢Â Ground rule ìˆ˜ì •ì‚¬í•­

- ê³µìœ  í”¼í”¼í‹°ë¥¼ í™œìš©í•´ ììœ ë¡­ê²Œ ìµëª…ìœ¼ë¡œ ì§ˆë¬¸ì„ ë‚¨ê¸´ë‹¤. - ìµëª… ì§ˆë¬¸í†¡ë°©ì— ì§ˆë¬¸ì„ ë‚¨ê¸´ë‹¤
    - ìµëª… ì¹´í†¡ë°© : [https://open.kakao.com/o/gLZi2Cyd](https://open.kakao.com/o/gLZi2Cyd)
    - !ìµëª… ì¹´í†¡ë°© : [https://open.kakao.com/o/gQh02Cyd](https://open.kakao.com/o/gQh02Cyd)

# í›„ê¸°